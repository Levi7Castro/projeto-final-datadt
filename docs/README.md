# üöÄ Projeto Final: Da Opera√ß√£o ao Preditivo ‚Äì Vendas e Finan√ßas


## üéØ 1. Vis√£o Geral e Contexto de Neg√≥cio

Este projeto de Engenharia de Dados e Machine Learning visa modernizar a tomada de decis√£o de uma empresa de varejo, que busca evoluir de relat√≥rios puramente operacionais para uma gest√£o baseada em dados (Data-Driven).

A solu√ß√£o transforma dados transacionais brutos de **Vendas**, **Contas a Receber** e **Contas a Pagar** em insights preditivos e acion√°veis, com foco na otimiza√ß√£o das √°reas de marketing e cobran√ßa.

### üîë Objetivos Principais

* **Pipeline ETL:** Constru√ß√£o de um pipeline robusto em Python (Extra√ß√£o ‚Üí Transforma√ß√£o ‚Üí Qualidade ‚Üí Carga).
* **Data Warehouse (DW):** Modelagem de um DW em esquema estrela (*Star Schema*) no SQL Server, integrando dom√≠nios de Vendas e Financeiro.
* **Data Lake:** Disponibiliza√ß√£o de um *slice* anal√≠tico particionado em formato Parquet no Hadoop (HDFS).
* **Machine Learning (ML):** Treinamento e avalia√ß√£o de modelos preditivos para resolver problemas de neg√≥cio, tais como:
    * *Classifica√ß√£o:* Probabilidade de Recompra em 90 dias.
    * *Classifica√ß√£o:* Risco de Atraso no Pagamento.

---

## üß± 2. Arquitetura da Solu√ß√£o

O projeto implementa um fluxo de dados *End-to-End*, desde a origem transacional at√© o consumo anal√≠tico e preditivo.

### 2.1 Tecnologias Utilizadas

| Camada | Ferramenta | Descri√ß√£o |
| :--- | :--- | :--- |
| **Origem** | PostgreSQL | Base de dados transacional (OLTP). |
| **Orquestra√ß√£o** | Apache Airflow | Agendamento e automa√ß√£o de DAGs reprodut√≠veis. |
| **ETL & Processamento** | Python | Uso de Pandas, SQLAlchemy e PyArrow para manipula√ß√£o de dados. |
| **Data Warehouse** | SQL Server | Banco de dados anal√≠tico (OLAP). |
| **Data Lake** | Hadoop/HDFS | Armazenamento de *slices* em Parquet particionado. |
| **Machine Learning** | Scikit-learn, XGBoost | Treinamento, avalia√ß√£o e scoring de modelos. |
| **Versionamento** | Git | Controle de vers√£o do c√≥digo-fonte. |

### 2.2 Estrutura do Reposit√≥rio

```bash
projeto-final-datadt/
‚îú‚îÄ‚îÄ airflow/            # DAGs e scripts para orquestra√ß√£o no Apache Airflow
‚îÇ   ‚îú‚îÄ‚îÄ dags/
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ etl/                # Scripts Python (Extra√ß√£o, Transforma√ß√£o e Carga)
‚îú‚îÄ‚îÄ dw/                 # Scripts SQL (DDL e DML) para o SQL Server
‚îú‚îÄ‚îÄ lake/               # Scripts para gera√ß√£o do Slice Parquet no HDFS
‚îú‚îÄ‚îÄ ml/                 # Notebooks de modelagem e scripts de scoring
‚îú‚îÄ‚îÄ docs/               # Diagramas de arquitetura e documenta√ß√£o de projeto
‚îú‚îÄ‚îÄ .env                # Vari√°veis de ambiente (N√ÉO VERSIONADO)


-----

## üõ†Ô∏è 3. Configura√ß√£o e Execu√ß√£o

### 3.1 Credenciais da Fonte de Dados (Seguran√ßa)

‚ö†Ô∏è **ATEN√á√ÉO:** As credenciais de acesso ao banco transacional **n√£o devem** ser "commitadas" diretamente no c√≥digo. Utilize um arquivo `.env` na raiz do projeto para armazen√°-las com seguran√ßa.

Exemplo de estrutura do arquivo `.env`:

```ini
DB_HOST=postgresql-datadt.alwaysdata.net
DB_NAME=datadt_digital_corporativo
DB_USER=seu_usuario_aqui
DB_PASS=sua_senha_aqui
```

### 3.2 Setup do Ambiente Local

1.  **Clone o Reposit√≥rio:**

    ```bash
    git clone [link-do-seu-repositorio]
    cd projeto-final-datadt
    ```

2.  **Crie e Ative o Ambiente Virtual:**

    ```bash
    python -m venv .venv

    # Windows
    .\.venv\Scripts\activate


    ```

3.  **Instale as Depend√™ncias:**

    ```bash
    pip install -r airflow/requirements.txt
    ```

### 3.3 Ordem de Execu√ß√£o do Pipeline

Para garantir a reprodutibilidade do projeto ("do zero ao fim"), execute os passos na seguinte ordem (manualmente ou via Airflow):

1.  **Cria√ß√£o do DW:** Executar scripts DDL localizados em `dw/` para criar as tabelas no SQL Server.
2.  **Carga ETL:** Rodar o script principal em `etl/` para realizar a carga (Staging ‚Üí DW).
3.  **Exporta√ß√£o para o Lake:** Rodar o script em `lake/` para gerar os arquivos Parquet no HDFS.
4.  **Modelagem ML:** Executar os notebooks em `ml/` para treinar e avaliar o modelo preditivo.

-----

## üìà 4. Crit√©rios de Avalia√ß√£o e Entreg√°veis

| Entreg√°vel | Detalhes | M√©tricas de Qualidade |
| :--- | :--- | :--- |
| **Modelagem DW** | Modelo de Dados e scripts SQL (DDL/DML) na pasta `dw/`. | Integridade referencial e normaliza√ß√£o dimensional. |
| **Carga ETL** | Scripts Python funcionais em `etl/`. | Reprodutibilidade e tratamento de erros. |
| **Data Lake** | Arquivos Parquet em `hdfs/` ou simulado localmente. | Particionamento eficiente (ex: ano/m√™s). |
| **Modelo Preditivo** | Notebooks de modelagem na pasta `ml/`. | **Classifica√ß√£o:** AUC, Recall@10%. <br> **Regress√£o:** MAE, sMAPE. |
| **Exporta√ß√£o ML** | Modelo treinado (`.joblib`) e pipeline de scoring. | Capacidade de predi√ß√£o em novos dados. |
| **Documenta√ß√£o** | Diagrama de arquitetura em `docs/`. | Clareza nas "assun√ß√µes" de neg√≥cio. |
| **Relat√≥rio Final** | An√°lise de KPIs e Plano de A√ß√£o. | Relev√¢ncia dos insights para o neg√≥cio. |

